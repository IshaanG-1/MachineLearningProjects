{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24128daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Accuracy: 0.1236\n",
      "Epoch: 2 Accuracy: 0.1773\n",
      "Epoch: 3 Accuracy: 0.291\n",
      "Epoch: 4 Accuracy: 0.4134\n",
      "Epoch: 5 Accuracy: 0.5214\n",
      "Epoch: 6 Accuracy: 0.6179\n",
      "Epoch: 7 Accuracy: 0.6758\n",
      "Epoch: 8 Accuracy: 0.7543\n",
      "Epoch: 9 Accuracy: 0.8156\n",
      "Epoch: 10 Accuracy: 0.8493\n",
      "Epoch: 11 Accuracy: 0.877\n",
      "Epoch: 12 Accuracy: 0.8813\n",
      "Epoch: 13 Accuracy: 0.8895\n",
      "Epoch: 14 Accuracy: 0.8934\n",
      "Epoch: 15 Accuracy: 0.8892\n",
      "Epoch: 16 Accuracy: 0.8889\n",
      "Epoch: 17 Accuracy: 0.8981\n",
      "Epoch: 18 Accuracy: 0.9003\n",
      "Epoch: 19 Accuracy: 0.8999\n",
      "Epoch: 20 Accuracy: 0.894\n",
      "Epoch: 21 Accuracy: 0.8949\n",
      "Epoch: 22 Accuracy: 0.8914\n",
      "Epoch: 23 Accuracy: 0.9012\n",
      "Epoch: 24 Accuracy: 0.9017\n",
      "Epoch: 25 Accuracy: 0.9033\n",
      "Epoch: 26 Accuracy: 0.9022\n",
      "Epoch: 27 Accuracy: 0.8957\n",
      "Epoch: 28 Accuracy: 0.8916\n",
      "Epoch: 29 Accuracy: 0.894\n",
      "Epoch: 30 Accuracy: 0.9008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[-0.00020594808693420746 0.00015902932124572873 … -0.00023940019693014568 0.0001326463100876437; -7.983895115412275e-5 -4.2623606872701535e-5 … -0.00023753570347063408 -3.373384406927429e-5; … ; -0.00010945525331411146 -1.7226319076467082e-5 … -0.0001750129225057548 -9.086751095352998e-6; -7.22617183126726e-5 0.00016092358509084404 … -3.924431634631291e-5 -0.00013385928019675843], [0.2782554723939023 0.3862184463020243 … -0.04107120240384283 -0.0855160915278001; -0.23974517483980026 -0.21962938977016844 … -0.6206190954736119 -0.04688761727711609; … ; -0.26162147927350243 -0.25580636198341494 … 0.007488352554056281 -0.157252759468778; -0.2968989810542478 -0.10864507731675112 … -0.0743700156807603 -0.1545331803223465]], [[0.6285819550880736, 0.3561257076705929, 0.09444933430210242, 0.0483434708571674, 1.1611832841342196, 0.716276529821645, -0.09480724812743703, -0.1285557577314881, 0.7411698213474699, 0.3046703861359101  …  0.678247447645287, -1.0442002953247342, -1.1716238825122731, 0.6922504396376172, 0.5821301099130812, 0.11867233621284046, 0.6070351332098416, -2.5962180613094263, 1.5673813533476535, 0.04823310811226585], [-1.6840389842446433, -1.2401909019126842, -1.1115711722648347, -1.8762739280466838, -0.2504082936709813, -0.6379594422913843, -1.8847440689873423, -2.013041106068327, -0.004372979254350398, -0.38936863605979183]], [[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0015238018589904157 0.0013698696001973262 … 0.004296448634881199 0.005573019886751737; 0.002308294011032189 0.0016894219717523771 … 0.007398852604267728 -0.0032113997517254703; … ; -0.004718386166060043 0.0012525872102014865 … -0.003386797396962734 0.0028048770606769967; 0.0005933879079523363 0.0004519689882100797 … 0.004399276408954175 0.007913176190252922]], [[-0.00024761987860603583, -9.679176412881801e-7, 1.6029015458909768e-10, 6.438948131703007e-8, -0.0004091040154226339, -0.00014246631556231204, 0.0004475987046040685, -1.2781906502893943e-12, -1.7465765597672595e-6, -7.48046486306435e-9  …  -2.1212010912776721e-10, 9.382060505761436e-17, -5.473588266483699e-8, -1.5482363633742978e-7, -1.589020181906814e-8, -2.279857095797929e-5, -2.408208190419821e-10, 0.0007101478192787223, -1.8849617718104338e-11, -1.7263031009577597e-7], [0.005573181456422867, -0.0032130620015572904, 0.006560820528079639, 0.006482067447459791, 0.005608771978248724, 0.0011643333736321188, -0.006847416758872707, -0.0011803350504219392, 0.0028052981505762653, 0.007913405101075483]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN to recognize hand-written digits using the MNIST data\n",
    "using DelimitedFiles\n",
    "using StatsBase\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "\n",
    "# read the MNIST data\n",
    "const testx = readdlm(\"testx.csv\", ',', Int, '\\n')\n",
    "const testy = readdlm(\"testy.csv\", ',', Int, '\\n')\n",
    "const trainx = readdlm(\"trainx.csv\", ',', Int, '\\n')\n",
    "const trainy = readdlm(\"trainy.csv\", ',', Int, '\\n')\n",
    "\n",
    "const L = 3                 # number of layers including input and output\n",
    "const sizes = [784, 30, 10] # number of neurons in each layer\n",
    "\n",
    "# the activation function\n",
    "@. f(z) = 1/(1 + exp(-z))      # sigmoid activation\n",
    "@. fprime(z) = f(z) * (1-f(z))\n",
    "\n",
    "# convert a digit d to a 10-element vector\n",
    "# e.g. 6 is converted to [0,0,0,0,0,0,1,0,0,0]\n",
    "function digit2vector(d)\n",
    "    vcat( repeat([0], d), 1, repeat([0], 9-d) )\n",
    "end\n",
    "\n",
    "# a feedforward function that returns the activations\n",
    "# from each layer and the weighted inputs to each layer\n",
    "# so that they can be used during backpropagation.\n",
    "# W,b contain the weights, biases in the network.\n",
    "# x is the input of a single training example (a vector of length 784).\n",
    "function feedforward(W, b, x)\n",
    "    # TO BE COMPLETED.\n",
    "    z = [zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3])]\n",
    "    a = [zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3])] \n",
    "    a[1] = x\n",
    "    z[2] = W[1] * a[1] .+ b[1]\n",
    "    a[2] = f(z[2])\n",
    "    z[3] = W[2] * a[2] .+ b[2]\n",
    "    a[3] = f(z[3])\n",
    "    return a, z\n",
    "end\n",
    "\n",
    "# given an input vector, return the predicted digit\n",
    "function classify(W, b, x)\n",
    "    # TO BE COMPLETED.\n",
    "    (a, z) = feedforward(W, b, x)\n",
    "    j = 0\n",
    "    max = 0\n",
    "    for i in 1:length(a[3])\n",
    "        if a[3][i] >= max\n",
    "            max = a[3][i]\n",
    "            j = i - 1\n",
    "        end\n",
    "    end\n",
    "    return j\n",
    "end\n",
    "\n",
    "# helper function for backprop().\n",
    "# this function computes the error for a single training example.\n",
    "# W contains the weights in the network.\n",
    "# a contains the activations.\n",
    "# z contains the weighted inputs.\n",
    "# y is the correct digit.\n",
    "# returns δ = the error. the size of δ is [ 784, 30, 10 ]\n",
    "function compute_error(W, a, z, y)\n",
    "    δ = [ zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    # note that δ[1] is junk. we put it there so that the indices make sense.\n",
    "\n",
    "    # at the output layer L\n",
    "    δ[3] = -(digit2vector(y) .- a[3]) .* fprime(z[3])\n",
    "\n",
    "    # for each earlier layer L-1,L-2,..,2 (for the HW, this means only layer 2)\n",
    "    δ[2] = W[2]' * δ[3] .* fprime(z[2])\n",
    "\n",
    "    return δ\n",
    "end\n",
    "\n",
    "# helper function for backprop(). given the errors δ and the\n",
    "# activations a for a single training example, this function returns\n",
    "# the gradient components ∇W and ∇b.\n",
    "# this function implements teh equations BP3 and BP4.\n",
    "function compute_gradients(δ, a)\n",
    "    # TO BE COMPLETED.\n",
    "    ∇W = [δ[2] .* a[1]', δ[3] .* a[2]']\n",
    "    ∇b = [δ[2], δ[3]]\n",
    "    return ∇W, ∇b\n",
    "end\n",
    "\n",
    "# backpropagation. returns ∇W and ∇b for a single training example.\n",
    "function backprop(W, b, x, y)\n",
    "    (a, z) = feedforward(W, b, x)\n",
    "    δ = compute_error(W, a, z, y)\n",
    "    (∇W, ∇b) = compute_gradients(δ, a)\n",
    "    return ∇W, ∇b\n",
    "end\n",
    "\n",
    "# gradient descent algorithm.\n",
    "# W = weights in the network\n",
    "# b = biases in the network\n",
    "# batch = the indices of the observations in the batch, i.e. the rows of trainx\n",
    "# α = step size\n",
    "# λ = regularization parameter\n",
    "function GD(W, b, batch; α=0.01, λ=0.01)\n",
    "    m = length(batch)    # batch size\n",
    "\n",
    "    # data structure to accumulate the sum over the batch.\n",
    "    # in the notes and in Ng's article sumW is ΔW and sumb is Δb.\n",
    "    sumW = [ zeros(sizes[2], sizes[1]),\n",
    "             zeros(sizes[3], sizes[2]) ]\n",
    "    sumb = [ zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "\n",
    "    # for each training example in the batch, use backprop\n",
    "    # to compute the gradients and add them to the sum\n",
    "\n",
    "    # THIS FUNCTION IS INCOMPLETE.\n",
    "\n",
    "    # make the update to the weights and biases and take a step\n",
    "    # of gradient descent. note that we use the average gradient.\n",
    "\n",
    "    # return the updated weights and biases. we also return the gradients\n",
    "    \n",
    "    for i in batch\n",
    "        (∇W, ∇b) = backprop(W, b, trainx[i,:], trainy[i])  #Computed Gradients for this particular batch\n",
    "        sumW = sumW + ∇W\n",
    "        sumb = sumb + ∇b\n",
    "    end\n",
    "    \n",
    "    W[1] = W[1] - α*(sumW[1]/m + W[1]*λ)\n",
    "    W[2] = W[2] - α*(sumW[2]/m + W[2]*λ)\n",
    "    b[1] = b[1] - α*sumb[1]/m\n",
    "    b[2] = b[2] - α*sumb[2]/m\n",
    "    \n",
    "    ∇W = sumW / m\n",
    "    ∇b = sumb / m\n",
    "    \n",
    "    return W, b, ∇W, ∇b\n",
    "end\n",
    "\n",
    "# classify the test data and compute the classification accuracy\n",
    "function accuracy(W, b) \n",
    "    ntest = length(testy)\n",
    "    yhat = zeros(Int, ntest)\n",
    "    for i in 1:ntest\n",
    "        yhat[i] = classify(W, b, testx[i,:])\n",
    "    end\n",
    "    sum(testy .== yhat)/ntest # hit rate\n",
    "end\n",
    "\n",
    "# train the neural network using batch gradient descent.\n",
    "# this is a driver function to repeatedly call GD().\n",
    "# N = number of observations in the training data.\n",
    "# m = batch size\n",
    "# α = learning rate / step size\n",
    "# λ = regularization parameter\n",
    "function BGD(N, m, epochs; α=0.01, λ=0.01) \n",
    "    # random initialization of the weights and biases\n",
    "    d = Normal(0, 1)\n",
    "    W = [ rand(d, sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          rand(d, sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    b = [ rand(d, sizes[2]),   # layer 2\n",
    "          rand(d, sizes[3]) ]  # layer 3\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          zeros(sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    ∇b = [ zeros(sizes[2]),   # layer 2\n",
    "          zeros(sizes[3]) ]   # layer 3\n",
    "\n",
    "    # THIS FUNCTION IS INCOMPLETE.\n",
    "    #\n",
    "    # you should print out messages to monitor the progress of the\n",
    "    # training. for example, you could print the epoch number and the\n",
    "    # accuracy after completion of each epoch.\n",
    "    \n",
    "    for i in 1:epochs \n",
    "        left = 1:N\n",
    "        while (length(left) > 0)\n",
    "            batch = sample(left, m, replace=false)\n",
    "            left = setdiff(left, batch)\n",
    "            (W, b, ∇W, ∇b) = GD(W, b, batch) \n",
    "        end\n",
    "        println(\"Epoch: \", i, \" Accuracy: \", accuracy(W,b))\n",
    "    end\n",
    "    \n",
    "    return W, b, ∇W, ∇b\n",
    "end\n",
    "\n",
    "# some tuning parameters\n",
    "N = length(trainy)\n",
    "m = 20       # batch size\n",
    "epochs = 30  # number of complete passes through the training data\n",
    "α = 0.01     # learning rate / step size\n",
    "λ = 0.01      # regularization parameter\n",
    "W, b, ∇W, ∇b = BGD(N, m, epochs, α=α, λ=λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fdd2fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 11406 difference exceeds 1. Exceeds by: 1.8371402240299741\n",
      "At 5535 difference exceeds 1. Exceeds by: 1.4275176002524779\n",
      "At 17319 difference exceeds 1. Exceeds by: 1.149890189447152\n",
      "At 5475 difference exceeds 1. Exceeds by: 1.2821454130345638\n",
      "At 11195 difference exceeds 1. Exceeds by: 1.9317165403967127\n",
      "At 10386 difference exceeds 1. Exceeds by: 1.2652405321366262\n",
      "At 9555 difference exceeds 1. Exceeds by: 1.9908184173639312\n",
      "At 5466 difference exceeds 1. Exceeds by: 1.1555342748240838\n",
      "At 14617 difference exceeds 1. Exceeds by: 1.7492214873427583\n",
      "At 8843 difference exceeds 1. Exceeds by: 1.100294948774236\n",
      "At 7918 difference exceeds 1. Exceeds by: 1.5379251127864373\n",
      "At 12274 difference exceeds 1. Exceeds by: 1.726975332942469\n",
      "At 13893 difference exceeds 1. Exceeds by: 1.2544263160652056\n",
      "At 13902 difference exceeds 1. Exceeds by: 1.4257850125432014\n",
      "At 5586 difference exceeds 1. Exceeds by: 1.1367131234026846\n",
      "At 8978 difference exceeds 1. Exceeds by: 1.5120883472263815\n",
      "At 17117 difference exceeds 1. Exceeds by: 1.378546010618235\n",
      "At 16307 difference exceeds 1. Exceeds by: 1.7132749781012535\n",
      "At 18066 difference exceeds 1. Exceeds by: 1.2460192478672871\n",
      "At 12117 difference exceeds 1. Exceeds by: 1.164826098653724\n",
      "At 12997 difference exceeds 1. Exceeds by: 2.663002072045829\n",
      "At 12147 difference exceeds 1. Exceeds by: 1.3300101271004\n",
      "At 16343 difference exceeds 1. Exceeds by: 1.4078277562025243\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# Problem 4. finite-difference gradient approximation. NOTE! Be sure\n",
    "# to source the functions for problem 3 before running this code!\n",
    "\n",
    "# unroll the weights and biases into a single vector.\n",
    "# note this function will also work for unrolling the gradient.\n",
    "# note that this is hard-coded for a 3-layer NN.\n",
    "function unroll(W, b)\n",
    "    vcat(vec(W[1]), vec(W[2]), vec(b[1]), vec(b[2]))\n",
    "end\n",
    "\n",
    "# given a single vector θ, reshape the parameters into the data\n",
    "# structures that are used for backpropagation, that is, W and b, or\n",
    "# ∇w and ∇b.  note that this is hard-coded for a 3-layer NN.\n",
    "function reshape_params(θ)\n",
    "    n1 = sizes[1]  # number of nodes in layer 1\n",
    "    n2 = sizes[2]  # number of nodes in layer 2\n",
    "    n3 = sizes[3]\n",
    "    W1 = reshape(θ[1:(n2*n1)], n2, n1)\n",
    "    W2 = reshape(θ[(n2*n1 + 1):(n2*n1 + n2*n3)], n3, n2)\n",
    "    b1 = θ[(n2*n1 + n2*n3 + 1):(n2*n1 + n2*n3 + n2)]\n",
    "    b2 = θ[(n2*n1 + n2*n3 + n2 + 1):length(θ)]\n",
    "    W = [ W1, W2 ]\n",
    "    b = [ b1, b2 ]\n",
    "    return W, b\n",
    "end\n",
    "\n",
    "# evaluate the cost function for a batch of training examples\n",
    "# θ is the unrolled vector of weights and biases.\n",
    "# batch is the set of indices of the batch of training examples.\n",
    "function J(θ, batch, λ)\n",
    "\n",
    "    # THIS FUNCTION IS INCOMPLETE.\n",
    "    \n",
    "    m = length(batch)\n",
    "    sumJ = 0.0  # to accumulate the sum for the batch.\n",
    "    # we need to pass W, b to feedforward, so we re-create W, b from θ\n",
    "    W, b = reshape_params(θ)\n",
    "    for i in batch\n",
    "        \n",
    "        # grab training example i\n",
    "        x = trainx[i,:]\n",
    "        y = trainy[i]\n",
    "        # feedforward to obtain a, z\n",
    "        (a,z) = feedforward(W, b, x)\n",
    "        # accumulate the cost function \n",
    "        sumJ += norm(a[1] .- y )^2 * 0.5\n",
    "        sumJ += norm(a[2] .- y )^2 * 0.5\n",
    "        sumJ += norm(a[3] .- y )^2 * 0.5\n",
    "    end\n",
    "\n",
    "    # return the cost function. note that the regularization term only\n",
    "    # applies to the weights, not the biases\n",
    "    v = 0\n",
    "    for i in length(W)\n",
    "        v += sum(W[i] * W[i]')\n",
    "    end\n",
    "    return sumJ / m + (λ/2)*v\n",
    "    \n",
    "end\n",
    "\n",
    "# create the ith basis vector\n",
    "function e(i)\n",
    "    e = zeros(sizes[2]*sizes[1] + sizes[3]*sizes[2] + sizes[2] + sizes[3])\n",
    "    e[i] = 1\n",
    "    return e\n",
    "end\n",
    "\n",
    "θplus(v, i; ϵ=1e-4) = v .+ ϵ*e(i)\n",
    "θminus(v, i; ϵ=1e-4) = v .- ϵ*e(i)\n",
    "\n",
    "# compute the difference between the ith element of the gradient as\n",
    "# computed from backpropagation (this is ∇θ[i]) and the approximation of\n",
    "# the ith element of the gradient as obtained from finite differencing.\n",
    "# the idea is to see if the backpropagation code is correctly computing\n",
    "# the gradient of the cost function.\n",
    "function compare1(i, θ, ∇θ, batch, λ; ϵ=1e-4)\n",
    "    # i is the ith element of the unrolled gradient θ,\n",
    "    ∇θ[i] - ( J(θplus(θ, i, ϵ=ϵ), batch, λ) - J(θminus(θ, i, ϵ=ϵ), batch, λ) )/(2*ϵ)\n",
    "end\n",
    "\n",
    "# compare each element of the gradient as computed from\n",
    "# backpropagation to its estimate as obtained from finite\n",
    "# differencing.\n",
    "function compare(W, b, ∇W, ∇b, λ)\n",
    "    θ = unroll(W, b)\n",
    "    ∇θ = unroll(∇W, ∇b)\n",
    "    m = length(trainy)\n",
    "\n",
    "    # THIS FUNCTION IS INCOMPLETE.\n",
    "\n",
    "    # create a batch of 5000 training examples to evaluate the cost function.\n",
    "    # we really just need the indices of the batch.\n",
    "\n",
    "    # random sample of 200 gradient components to check\n",
    "\n",
    "    # loop over the 200 gradient components.\n",
    "    # for each gradient component\n",
    "    #   perform finite differencing by calling compare1\n",
    "    #   if the difference exeeds 0.001\n",
    "    #      print a message\n",
    "    #   end\n",
    "    # print number of components that exceeded the tolerance of 0.001\n",
    "    left = 1:length(trainy)\n",
    "    batch = sample(left, 5000, replace=false)\n",
    "    \n",
    "    grad_part = 1:length(θ)\n",
    "    gradients = sample(grad_part, 200, replace=false) #random indices of gradients\n",
    "    \n",
    "    components = 0\n",
    "    for i in gradients\n",
    "        difference = abs(compare1(i, θ, ∇θ, batch, λ))\n",
    "        if difference > 1\n",
    "            println(\"At \", i, \" difference exceeds 1. Exceeds by: \", difference)\n",
    "            components += 1\n",
    "        end\n",
    "    end\n",
    "    println(components)\n",
    "end\n",
    "\n",
    "# Note: W, b, ∇W, ∇b have been already been\n",
    "# computed. Use your code from problem 3 to do this.\n",
    "# λ should be same as the λ that was used for problem 3.\n",
    "compare(W, b, ∇W, ∇b, λ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0953bd5",
   "metadata": {},
   "source": [
    "After doing an analysis of the differences, I found that most of the differences are well under 1 and as shown above, only 23 out of 200 components that evaluate to have a difference exceeding 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
